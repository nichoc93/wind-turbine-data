{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silver Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any entries where data is missing in any of the columns\n",
    "def extract_rows_with_null_data(df_input: DataFrame) -> DataFrame:\n",
    "    \"\"\" \n",
    "        Removes any invalid entries from our dataframe if they contain null values\n",
    "    \"\"\"\n",
    "\n",
    "    df_filtered = df_input.filter(\n",
    "        F.col('timestamp').isNotNull() &\n",
    "        F.col(\"turbine_id\").isNotNull() & \n",
    "        F.col(\"wind_speed\").isNotNull() &\n",
    "        F.col(\"wind_direction\").isNotNull() & \n",
    "        F.col(\"power_output\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract any outliers which fall outside of some set ranges\n",
    "def extract_outliers(df_input: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows where any specified column values fall outside their defined min and max limits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply filter conditions for each column\n",
    "    df_filtered = df_input.filter(\n",
    "        (F.col('wind_speed') >= 0.0) & (F.col('wind_speed') <= 18.0) &\n",
    "        (F.col('wind_direction') >= 0) & (F.col('wind_direction') <= 359) &\n",
    "        (F.col('power_output') >= 1.0) & (F.col('power_output') <= 9.0)\n",
    "    )\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_statistics_gold_df(input_df: DataFrame) -> DataFrame:\n",
    "    ''' \n",
    "        Takes our silver dataframe and generates summary statistics for each turbine\n",
    "        Result is a dataframe that shows the min, max and average power output for each turbine on each day \n",
    "    '''\n",
    "\n",
    "    df_with_date = input_df\\\n",
    "                    .withColumn(\"date\", F.col(\"timestamp\").cast(DateType()))\n",
    "\n",
    "    # Group by 'turbine_id' and 'date', and calculate summary statistics\n",
    "    summary_df = df_with_date.groupBy(\"turbine_id\", \"date\").agg(\n",
    "        F.min(\"power_output\").alias(\"min_power_output\"),\n",
    "        F.max(\"power_output\").alias(\"max_power_output\"),\n",
    "        F.avg(\"power_output\").alias(\"avg_power_output\")\n",
    "    )\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anomalies_gold_df(input_df: DataFrame) -> DataFrame:\n",
    "    ''' \n",
    "    Identifies anomalies in power output for each turbine, defined as outputs \n",
    "    outside of 2 standard deviations from the mean for each day.\n",
    "    '''\n",
    "\n",
    "    # Convert 'timestamp' to 'date' and calculate our mean and standard deviation over our window of turbine_id and date\n",
    "    df_with_date = input_df.withColumn(\"date\", F.col(\"timestamp\").cast(DateType()))\n",
    "    windowSpec = Window.partitionBy(\"turbine_id\", \"date\")\n",
    "    stats_df = df_with_date.withColumn(\"mean_power_output\", F.avg(\"power_output\").over(windowSpec)) \\\n",
    "                           .withColumn(\"stddev_power_output\", F.stddev(\"power_output\").over(windowSpec))\n",
    "\n",
    "    # Define the range for normal data (mean Â± 2 * stddev)\n",
    "    stats_df = stats_df.withColumn(\"lower_bound\", F.col(\"mean_power_output\") - 2 * F.col(\"stddev_power_output\")) \\\n",
    "                       .withColumn(\"upper_bound\", F.col(\"mean_power_output\") + 2 * F.col(\"stddev_power_output\"))\n",
    "\n",
    "    # Identify anomalies as those outside the bounds\n",
    "    anomalies_df = stats_df.filter(\n",
    "        (F.col(\"power_output\") < F.col(\"lower_bound\")) | (F.col(\"power_output\") > F.col(\"upper_bound\"))\n",
    "    )\n",
    "\n",
    "    return anomalies_df.select(\"timestamp\", \"turbine_id\", \"power_output\", \"mean_power_output\", \"stddev_power_output\", \"lower_bound\", \"upper_bound\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02effb55b36c50b793880894feb72bc6e2c19c827b038121749b9b9c87aaaab7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit ('collibri': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
