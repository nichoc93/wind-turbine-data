{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, DoubleType, DateType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing our functions that we can run tests on and creating a helper method to compare dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our functions to run our transforms with\n",
    "%run 'functions.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_dataframes_equal(df1, df2):\n",
    "    ''' \n",
    "        Helper Function to compare two dataframes are equal\n",
    "        Validates schemas, counts and rows are al equivalent, regardless of order\n",
    "    '''\n",
    "    # Check if the schemas match\n",
    "    assert df1.schema == df2.schema\n",
    "\n",
    "    # Compare row count\n",
    "    assert df1.count() == df2.count()\n",
    "\n",
    "    # Convert DataFrames to sets of tuples and compare\n",
    "    df1_rows = {tuple(row) for row in df1.collect()}\n",
    "    df2_rows = {tuple(row) for row in df2.collect()}\n",
    "\n",
    "    # Compare the sets of rows regardless of order\n",
    "    assert df1_rows == df2_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silver Function Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract_rows_with_null_data():\n",
    "    \"\"\"\n",
    "        Test function to validate our extract outliers function works as intended using some sample data\n",
    "    \"\"\"\n",
    "\n",
    "    # Define our schema\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"wind_direction\", IntegerType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Define some input data so we can test our transform works\n",
    "    # Typically each row is a condition we are testing for\n",
    "    sample_input_data = [\n",
    "        (None, 3, 5.4, 130, 2.6),  # None for timestamp\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), None, 5.4, 130, 2.6),\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 6, None, 112, 4.2),\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 6, None, 112, 4.2),\n",
    "        (datetime.strptime('2022-04-22 20:00:00', '%Y-%m-%d %H:%M:%S'), 7, 10.8, None, 3.9),\n",
    "        (datetime.strptime('2022-04-11 18:00:00', '%Y-%m-%d %H:%M:%S'), 9, 12.3, 98, None),\n",
    "        (datetime.strptime('2022-03-01 16:00:00', '%Y-%m-%d %H:%M:%S'), 4, 9.3, 120, 3.2),\n",
    "        (datetime.strptime('2022-04-11 11:00:00', '%Y-%m-%d %H:%M:%S'), 5, 8.5, 105, 4.5)\n",
    "    ]\n",
    "\n",
    "    # Our input data had 5 bad records and 2 good records, we expect only the good records to be kept\n",
    "    expected_output_data = [\n",
    "            (datetime.strptime('2022-03-01 16:00:00', '%Y-%m-%d %H:%M:%S'), 4, 9.3, 120, 3.2), \n",
    "            (datetime.strptime('2022-04-11 11:00:00', '%Y-%m-%d %H:%M:%S'), 5, 8.5, 105, 4.5)\n",
    "    ]\n",
    "\n",
    "    # Create Our Dataframes\n",
    "    test_df = spark.createDataFrame(sample_input_data, schema)\n",
    "    expected_output_df = spark.createDataFrame(expected_output_data, schema)\n",
    "\n",
    "    result_df = extract_rows_with_null_data(test_df)\n",
    "\n",
    "    assert_dataframes_equal(result_df, expected_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract_outliers():\n",
    "    \"\"\"\n",
    "    Test function to validate our extract_outliers function works as intended using some sample data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define our schema\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"wind_direction\", IntegerType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Define some input data for testing\n",
    "    sample_input_data = [\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 1, 17.0, 100, 3.0), # Within range\n",
    "        (datetime.strptime('2022-03-01 01:00:00', '%Y-%m-%d %H:%M:%S'), 2, 19.0, 150, 4.5), # Outlier in wind_speed\n",
    "        (datetime.strptime('2022-03-01 02:00:00', '%Y-%m-%d %H:%M:%S'), 3, 15.0, 360, 2.0), # Outlier in wind_direction\n",
    "        (datetime.strptime('2022-03-01 03:00:00', '%Y-%m-%d %H:%M:%S'), 4, 10.0, 200, 0.5)  # Outlier in power_output\n",
    "    ]\n",
    "\n",
    "    # Expected data after removing outliers\n",
    "    expected_output_data = [\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 1, 17.0, 100, 3.0) # Only this row should remain\n",
    "    ]\n",
    "\n",
    "    # Create DataFrames\n",
    "    test_df = spark.createDataFrame(sample_input_data, schema)\n",
    "    expected_output_df = spark.createDataFrame(expected_output_data, schema)\n",
    "\n",
    "    # Apply the extract_outliers function\n",
    "    result_df = extract_outliers(test_df)\n",
    "\n",
    "    # Use an assert function to compare the dataframes\n",
    "    assert_dataframes_equal(result_df, expected_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "test_extract_rows_with_null_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extract_outliers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_summary_statistics_gold_df():\n",
    "    \"\"\"\n",
    "    Test function to validate our generate_summary_statistics_gold_df function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define our schema\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"wind_direction\", IntegerType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Define some input data for testing\n",
    "    sample_input_data = [\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 1, 5.0, 100, 2.5),\n",
    "        (datetime.strptime('2022-03-01 01:00:00', '%Y-%m-%d %H:%M:%S'), 1, 6.0, 110, 3.0),\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 2, 7.0, 120, 4.5),\n",
    "        (datetime.strptime('2022-03-02 00:00:00', '%Y-%m-%d %H:%M:%S'), 1, 5.5, 130, 2.7),\n",
    "        (datetime.strptime('2022-03-02 01:00:00', '%Y-%m-%d %H:%M:%S'), 1, 6.5, 140, 3.2)\n",
    "    ]\n",
    "\n",
    "    # Define schema for expected output data\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"min_power_output\", DoubleType(), True),\n",
    "        StructField(\"max_power_output\", DoubleType(), True),\n",
    "        StructField(\"avg_power_output\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Expected summary statistics\n",
    "    expected_output_data = [\n",
    "        (1, datetime.strptime('2022-03-01', '%Y-%m-%d').date(), 2.5, 3.0, 2.75), # turbine 1 on 2022-03-01\n",
    "        (1, datetime.strptime('2022-03-02', '%Y-%m-%d').date(), 2.7, 3.2, 2.95), # turbine 1 on 2022-03-02\n",
    "        (2, datetime.strptime('2022-03-01', '%Y-%m-%d').date(), 4.5, 4.5, 4.5)  # turbine 2 on 2022-03-01\n",
    "    ]\n",
    "\n",
    "    # Create DataFrames\n",
    "    test_df = spark.createDataFrame(sample_input_data, schema)\n",
    "    expected_output_df = spark.createDataFrame(expected_output_data, schema=expected_schema)\n",
    "\n",
    "    # Apply the generate_summary_statistics_gold_df function\n",
    "    result_df = generate_summary_statistics_gold_df(test_df)\n",
    "\n",
    "    # Use an assert function to compare the dataframes\n",
    "    assert_dataframes_equal(result_df, expected_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_anomalies_gold_df():\n",
    "    \"\"\"\n",
    "    Test function to validate our generate_anomalies_gold_df function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input schema\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Define some input data for testing, including normal and anomalous values\n",
    "    sample_input_data = [\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 1, 2.5),\n",
    "        (datetime.strptime('2022-03-01 01:00:00', '%Y-%m-%d %H:%M:%S'), 1, 2.7),\n",
    "        (datetime.strptime('2022-03-01 02:00:00', '%Y-%m-%d %H:%M:%S'), 1, 2.7),\n",
    "        (datetime.strptime('2022-03-01 03:00:00', '%Y-%m-%d %H:%M:%S'), 1, 2.7),\n",
    "        (datetime.strptime('2022-03-01 04:00:00', '%Y-%m-%d %H:%M:%S'), 1, 2.7),\n",
    "        (datetime.strptime('2022-03-01 05:00:00', '%Y-%m-%d %H:%M:%S'), 1, 2.7),\n",
    "        (datetime.strptime('2022-03-01 06:00:00', '%Y-%m-%d %H:%M:%S'), 1, 10.4),  # Anomalous high value\n",
    "        (datetime.strptime('2022-03-01 00:00:00', '%Y-%m-%d %H:%M:%S'), 2, 3.5),\n",
    "        (datetime.strptime('2022-03-01 01:00:00', '%Y-%m-%d %H:%M:%S'), 2, 3.6)\n",
    "    ]\n",
    "\n",
    "    # Define schema for expected output data\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"turbine_id\", IntegerType(), True),\n",
    "        StructField(\"power_output\", DoubleType(), True),\n",
    "        StructField(\"mean_power_output\", DoubleType(), True),\n",
    "        StructField(\"stddev_power_output\", DoubleType(), True),\n",
    "        StructField(\"lower_bound\", DoubleType(), True),\n",
    "        StructField(\"upper_bound\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Expected output data after identifying anomalies\n",
    "    # Including only the row identified as an anomaly\n",
    "    expected_output_data = [\n",
    "        (datetime.strptime('2022-03-01 06:00:00', '%Y-%m-%d %H:%M:%S'), 1, 10.4, 3.7714285714285714, 2.9238754452007045, -2.0763223189728377, 9.61917946182998)  # Anomaly details\n",
    "    ]\n",
    "\n",
    "    # Create DataFrames\n",
    "    test_df = spark.createDataFrame(sample_input_data, schema)\n",
    "    expected_output_df = spark.createDataFrame(expected_output_data, schema=expected_schema)\n",
    "\n",
    "    # Apply the generate_anomalies_gold_df function\n",
    "    result_df = generate_anomalies_gold_df(test_df)\n",
    "\n",
    "    # Use an assert function to compare the dataframes\n",
    "    assert_dataframes_equal(result_df, expected_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generate_anomalies_gold_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generate_summary_statistics_gold_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unit tests completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02effb55b36c50b793880894feb72bc6e2c19c827b038121749b9b9c87aaaab7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit ('collibri': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
